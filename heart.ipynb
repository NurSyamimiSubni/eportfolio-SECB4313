{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NurSyamimiSubni/eportfolio-SECB4313/blob/main/heart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all library needed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "#confusion matrix visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ],
      "metadata": {
        "id": "O51yB0jSfMSw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Link notebook with google drive and access data from your personal Gdrive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "### 2.Set the data path for dataset and model location (ex: model_loc = \"/content/gdrive/My Drive/Dataset/\")\n",
        "dataset_dir = \"/content/gdrive/My Drive/Colab Notebooks/\" #insert the path here\n",
        "model_loc = \"/content/gdrive/My Drive/Colab Notebooks/\" #insert the path here\n",
        "\n",
        "print(os.listdir('/content/gdrive/My Drive/Colab Notebooks/'))\n",
        "data = pd.read_csv(dataset_dir+'heart.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqFKElsNfadC",
        "outputId": "0032d77b-8fd7-472b-9bb6-4a24f51bc4da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "['Copy of Welcome To Colaboratory (1)', 'Copy of Welcome To Colaboratory', 'Untitled', 'BreastCancer.ipynb', 'heart.csv', 'heart']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl9EyYbdiGI_",
        "outputId": "b2f2e804-5d4c-4b44-c471-f1ca092c8908"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 3. Insert Exploratory data analysis (EDA) steps to analyze and investigate datasets.\n",
        "data.head()"
      ],
      "metadata": {
        "id": "j4WNiOKLfd5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e9966ddd-a6cb-497f-cdb6-d2f606efe7a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa271833-acf2-496e-a5d8-8111071e5452\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa271833-acf2-496e-a5d8-8111071e5452')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aa271833-acf2-496e-a5d8-8111071e5452 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aa271833-acf2-496e-a5d8-8111071e5452');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f764cc68-3fe8-4fa5-bb01-43e5d826ab63\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f764cc68-3fe8-4fa5-bb01-43e5d826ab63')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f764cc68-3fe8-4fa5-bb01-43e5d826ab63 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 303,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          46,\n          66,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 94,\n        \"max\": 200,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          104,\n          123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51,\n        \"min\": 126,\n        \"max\": 564,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          277,\n          169\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22,\n        \"min\": 71,\n        \"max\": 202,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          159,\n          152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1610750220686348,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          1.9,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. What is the purpose of the code that sets a list of categorical variables\n",
        "### in a dataset and then casts those variables to the object data type using the astype() function?\n",
        "\n",
        "# To converts specific column in the Dataframe 'data' to the data type 'object' for each item in the list 'catagorialList'.\n",
        "# analysis, and interpretation of the data.\n",
        "catagorialList = ['sex','cp','fbs','restecg','exang','ca','thal']\n",
        "for item in catagorialList:\n",
        "    data[item] = data[item].astype('object') #casting to object"
      ],
      "metadata": {
        "id": "lpNHOx_-fh7c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ### 5. Create more data by categorical variable into indicator variables using 'get_dummies' function\n",
        "\n",
        "data = pd.get_dummies(data, drop_first=True) #complete your code here"
      ],
      "metadata": {
        "id": "eyEMb_M8fmeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fbddae-e307-440c-c3a8-b3c7ec366d32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n",
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n",
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n",
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n",
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n",
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n",
            "<ipython-input-6-b1a2422d1941>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True) #complete your code here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 6. Explain line 3,4 and 5 and print the shape of x and y\n",
        "\n",
        "y = data['target'].values #take target value\n",
        "y = y.reshape(y.shape[0],1) #reshape y array into 1 dimension\n",
        "x = data.drop(['target'],axis=1) #drop 'target' in x data\n",
        "y.shape"
      ],
      "metadata": {
        "id": "fhnsP8C8fm0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00472261-6309-4576-f9f7-ccdf34221d34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 7. Create a simple dataset and demonstrate the normalization code on the simple dataset\n",
        "\n",
        "data = pd.DataFrame({'A': [10, 20, 30], 'B': [100, 200, 300], 'C': [1000, 2000, 3000]})\n",
        "print('Original dataset:')\n",
        "print(data)\n",
        "\n",
        "minx = np.min(x)\n",
        "maxx = np.max(x)\n",
        "data_norm = (data - minx) / (maxx - minx)\n",
        "print('\\nNormalized dataset:')\n",
        "print(data_norm)"
      ],
      "metadata": {
        "id": "ykl00eCQfm91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0df905f-ebdf-4096-b3ee-b470b0f569ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset:\n",
            "    A    B     C\n",
            "0  10  100  1000\n",
            "1  20  200  2000\n",
            "2  30  300  3000\n",
            "\n",
            "Normalized dataset:\n",
            "    A   B   C  age  ca_1  ca_2  ca_3  ca_4  chol  cp_1  ...  oldpeak  \\\n",
            "0 NaN NaN NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...      NaN   \n",
            "1 NaN NaN NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...      NaN   \n",
            "2 NaN NaN NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...      NaN   \n",
            "\n",
            "   restecg_1  restecg_2  sex_1  slope  thal_1  thal_2  thal_3  thalach  \\\n",
            "0        NaN        NaN    NaN    NaN     NaN     NaN     NaN      NaN   \n",
            "1        NaN        NaN    NaN    NaN     NaN     NaN     NaN      NaN   \n",
            "2        NaN        NaN    NaN    NaN     NaN     NaN     NaN      NaN   \n",
            "\n",
            "   trestbps  \n",
            "0       NaN  \n",
            "1       NaN  \n",
            "2       NaN  \n",
            "\n",
            "[3 rows x 24 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 8. Describe the heart dataset after implementing the min max normalization\n",
        "#Normalize data (range 0 - 1)\n",
        "minx = np.min(x)\n",
        "maxx = np.max(x)\n",
        "x = (x - minx) / (maxx - minx)\n",
        "x.head()"
      ],
      "metadata": {
        "id": "YeMmIyeCfnHT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "eca67888-4f31-4365-c57c-63b351cc4264"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        age  trestbps      chol   thalach   oldpeak  slope  sex_1  cp_1  cp_2  \\\n",
              "0  0.708333  0.481132  0.244292  0.603053  0.370968    0.0    1.0   0.0   0.0   \n",
              "1  0.166667  0.339623  0.283105  0.885496  0.564516    0.0    1.0   0.0   1.0   \n",
              "2  0.250000  0.339623  0.178082  0.770992  0.225806    1.0    0.0   1.0   0.0   \n",
              "3  0.562500  0.245283  0.251142  0.816794  0.129032    1.0    1.0   1.0   0.0   \n",
              "4  0.583333  0.245283  0.520548  0.702290  0.096774    1.0    0.0   0.0   0.0   \n",
              "\n",
              "   cp_3  ...  restecg_1  restecg_2  exang_1  ca_1  ca_2  ca_3  ca_4  thal_1  \\\n",
              "0   1.0  ...        0.0        0.0      0.0   0.0   0.0   0.0   0.0     1.0   \n",
              "1   0.0  ...        1.0        0.0      0.0   0.0   0.0   0.0   0.0     0.0   \n",
              "2   0.0  ...        0.0        0.0      0.0   0.0   0.0   0.0   0.0     0.0   \n",
              "3   0.0  ...        1.0        0.0      0.0   0.0   0.0   0.0   0.0     0.0   \n",
              "4   0.0  ...        1.0        0.0      1.0   0.0   0.0   0.0   0.0     0.0   \n",
              "\n",
              "   thal_2  thal_3  \n",
              "0     0.0     0.0  \n",
              "1     1.0     0.0  \n",
              "2     1.0     0.0  \n",
              "3     1.0     0.0  \n",
              "4     1.0     0.0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-499ea2f6-08b7-4630-b1bf-5806980e67ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>thalach</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>sex_1</th>\n",
              "      <th>cp_1</th>\n",
              "      <th>cp_2</th>\n",
              "      <th>cp_3</th>\n",
              "      <th>...</th>\n",
              "      <th>restecg_1</th>\n",
              "      <th>restecg_2</th>\n",
              "      <th>exang_1</th>\n",
              "      <th>ca_1</th>\n",
              "      <th>ca_2</th>\n",
              "      <th>ca_3</th>\n",
              "      <th>ca_4</th>\n",
              "      <th>thal_1</th>\n",
              "      <th>thal_2</th>\n",
              "      <th>thal_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.481132</td>\n",
              "      <td>0.244292</td>\n",
              "      <td>0.603053</td>\n",
              "      <td>0.370968</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.283105</td>\n",
              "      <td>0.885496</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.178082</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.251142</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.520548</td>\n",
              "      <td>0.702290</td>\n",
              "      <td>0.096774</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-499ea2f6-08b7-4630-b1bf-5806980e67ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-499ea2f6-08b7-4630-b1bf-5806980e67ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-499ea2f6-08b7-4630-b1bf-5806980e67ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f81cc239-d210-4db8-96c0-7ea373694bbe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f81cc239-d210-4db8-96c0-7ea373694bbe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f81cc239-d210-4db8-96c0-7ea373694bbe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 9. Modify the code to split the dataset into train and test (train 70%, val 20% and test 10%).\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "# re-create train and validation set\n",
        "x_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "# train 70%, validation 20%, test 10%\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "axagLlisfnKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d791d51-c371-4e81-ebaa-72e46853f3ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(217, 21)\n",
            "(55, 21)\n",
            "(31, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 10. What is the purpose of each layer in the neural network created using the Sequential() function with 64, 32, and 1 neurons,\n",
        "### respectively, and softmax and sigmoid activation functions?\n",
        "\n",
        "model = Sequential() #Allow us to create model layer by layer\n",
        "model.add(Dense(64, input_dim=21, activation='softmax')) #Softmax turn number data into probabilities which sum to 1\n",
        "model.add(Dense(32, activation='softmax'))\n",
        "model.add(Dense(1, activation='sigmoid')) # produce probability value (number between 0 or 1)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5WqJIkwAf270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2977bc3-0ea3-4a60-e204-27f6c7e3c310"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 64)                1408      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3521 (13.75 KB)\n",
            "Trainable params: 3521 (13.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 11. This code compiles a neural network model with a mean squared error loss function, the Adam optimizer with a learning rate of 0.01,\n",
        "### and accuracy as a performance metric. What does each of these components mean, and how do they affect the model training and performance?\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999,epsilon=1e-07, amsgrad=False,name='Adam'),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "Yuwd7w23f3KF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start the model training\n",
        "output = []\n",
        "early = EarlyStopping(monitor='val_acc', patience=400, mode='auto')\n",
        "checkpoint = ModelCheckpoint(model_loc+\"heart_disease_best_model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='auto', save_freq='epoch')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.01, patience=100, verbose=1, mode='auto', min_lr=0.001)\n",
        "callbacks_list = [early]\n",
        "\n",
        "output = model.fit(x_train, y_train,validation_data=(x_val,y_val), epochs=1000, batch_size=16, verbose=1, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "VRh7unRCf3Qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7f94b9-b925-4588-ee9f-9a5ffab89151"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "14/14 [==============================] - 1s 19ms/step - loss: 0.2505 - acc: 0.4700 - val_loss: 0.2485 - val_acc: 0.5455\n",
            "Epoch 2/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2477 - acc: 0.5438 - val_loss: 0.2460 - val_acc: 0.5455\n",
            "Epoch 3/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2445 - acc: 0.5438 - val_loss: 0.2409 - val_acc: 0.5455\n",
            "Epoch 4/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2379 - acc: 0.5438 - val_loss: 0.2305 - val_acc: 0.5455\n",
            "Epoch 5/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2258 - acc: 0.6774 - val_loss: 0.2137 - val_acc: 0.7455\n",
            "Epoch 6/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2083 - acc: 0.8433 - val_loss: 0.1925 - val_acc: 0.8545\n",
            "Epoch 7/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1870 - acc: 0.8525 - val_loss: 0.1705 - val_acc: 0.8182\n",
            "Epoch 8/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1671 - acc: 0.8479 - val_loss: 0.1495 - val_acc: 0.8727\n",
            "Epoch 9/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1508 - acc: 0.8525 - val_loss: 0.1384 - val_acc: 0.8727\n",
            "Epoch 10/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1378 - acc: 0.8571 - val_loss: 0.1272 - val_acc: 0.8545\n",
            "Epoch 11/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1282 - acc: 0.8571 - val_loss: 0.1264 - val_acc: 0.8364\n",
            "Epoch 12/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1189 - acc: 0.8664 - val_loss: 0.1175 - val_acc: 0.8364\n",
            "Epoch 13/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1123 - acc: 0.8848 - val_loss: 0.1141 - val_acc: 0.8364\n",
            "Epoch 14/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1059 - acc: 0.8848 - val_loss: 0.1174 - val_acc: 0.8545\n",
            "Epoch 15/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1000 - acc: 0.8894 - val_loss: 0.1104 - val_acc: 0.8364\n",
            "Epoch 16/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0960 - acc: 0.8986 - val_loss: 0.1106 - val_acc: 0.8364\n",
            "Epoch 17/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0912 - acc: 0.9032 - val_loss: 0.1064 - val_acc: 0.8545\n",
            "Epoch 18/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0870 - acc: 0.9124 - val_loss: 0.1089 - val_acc: 0.8364\n",
            "Epoch 19/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0843 - acc: 0.9171 - val_loss: 0.1113 - val_acc: 0.8545\n",
            "Epoch 20/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0820 - acc: 0.9171 - val_loss: 0.1045 - val_acc: 0.8545\n",
            "Epoch 21/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0806 - acc: 0.9217 - val_loss: 0.1081 - val_acc: 0.8545\n",
            "Epoch 22/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0793 - acc: 0.9171 - val_loss: 0.1121 - val_acc: 0.8545\n",
            "Epoch 23/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0788 - acc: 0.9171 - val_loss: 0.1026 - val_acc: 0.8545\n",
            "Epoch 24/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0791 - acc: 0.9171 - val_loss: 0.1027 - val_acc: 0.8727\n",
            "Epoch 25/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0772 - acc: 0.9171 - val_loss: 0.1099 - val_acc: 0.8364\n",
            "Epoch 26/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0775 - acc: 0.9171 - val_loss: 0.1071 - val_acc: 0.8364\n",
            "Epoch 27/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0761 - acc: 0.9263 - val_loss: 0.1057 - val_acc: 0.8545\n",
            "Epoch 28/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0745 - acc: 0.9217 - val_loss: 0.1117 - val_acc: 0.8364\n",
            "Epoch 29/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0742 - acc: 0.9217 - val_loss: 0.1086 - val_acc: 0.8364\n",
            "Epoch 30/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0757 - acc: 0.9171 - val_loss: 0.1086 - val_acc: 0.8364\n",
            "Epoch 31/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0788 - acc: 0.9032 - val_loss: 0.1167 - val_acc: 0.8364\n",
            "Epoch 32/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0741 - acc: 0.9217 - val_loss: 0.1040 - val_acc: 0.8727\n",
            "Epoch 33/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0725 - acc: 0.9217 - val_loss: 0.1111 - val_acc: 0.8545\n",
            "Epoch 34/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0720 - acc: 0.9217 - val_loss: 0.1082 - val_acc: 0.8545\n",
            "Epoch 35/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0693 - acc: 0.9309 - val_loss: 0.1055 - val_acc: 0.8545\n",
            "Epoch 36/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0680 - acc: 0.9309 - val_loss: 0.1059 - val_acc: 0.8545\n",
            "Epoch 37/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0672 - acc: 0.9263 - val_loss: 0.1058 - val_acc: 0.8545\n",
            "Epoch 38/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0666 - acc: 0.9309 - val_loss: 0.1104 - val_acc: 0.8545\n",
            "Epoch 39/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0661 - acc: 0.9355 - val_loss: 0.1085 - val_acc: 0.8545\n",
            "Epoch 40/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0644 - acc: 0.9355 - val_loss: 0.1093 - val_acc: 0.8545\n",
            "Epoch 41/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0629 - acc: 0.9355 - val_loss: 0.1066 - val_acc: 0.8545\n",
            "Epoch 42/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0633 - acc: 0.9355 - val_loss: 0.1099 - val_acc: 0.8545\n",
            "Epoch 43/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0618 - acc: 0.9355 - val_loss: 0.1087 - val_acc: 0.8545\n",
            "Epoch 44/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0608 - acc: 0.9355 - val_loss: 0.1114 - val_acc: 0.8545\n",
            "Epoch 45/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0610 - acc: 0.9355 - val_loss: 0.1134 - val_acc: 0.8545\n",
            "Epoch 46/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0603 - acc: 0.9355 - val_loss: 0.1156 - val_acc: 0.8545\n",
            "Epoch 47/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0589 - acc: 0.9401 - val_loss: 0.1209 - val_acc: 0.8545\n",
            "Epoch 48/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0597 - acc: 0.9401 - val_loss: 0.1161 - val_acc: 0.8545\n",
            "Epoch 49/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0584 - acc: 0.9355 - val_loss: 0.1201 - val_acc: 0.8545\n",
            "Epoch 50/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0582 - acc: 0.9401 - val_loss: 0.1222 - val_acc: 0.8545\n",
            "Epoch 51/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0579 - acc: 0.9401 - val_loss: 0.1231 - val_acc: 0.8545\n",
            "Epoch 52/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0578 - acc: 0.9401 - val_loss: 0.1283 - val_acc: 0.8364\n",
            "Epoch 53/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0570 - acc: 0.9401 - val_loss: 0.1269 - val_acc: 0.8545\n",
            "Epoch 54/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0573 - acc: 0.9401 - val_loss: 0.1346 - val_acc: 0.8364\n",
            "Epoch 55/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0562 - acc: 0.9401 - val_loss: 0.1331 - val_acc: 0.8364\n",
            "Epoch 56/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0562 - acc: 0.9447 - val_loss: 0.1385 - val_acc: 0.8364\n",
            "Epoch 57/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0555 - acc: 0.9401 - val_loss: 0.1376 - val_acc: 0.8364\n",
            "Epoch 58/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0534 - acc: 0.9447 - val_loss: 0.1410 - val_acc: 0.8182\n",
            "Epoch 59/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0528 - acc: 0.9447 - val_loss: 0.1447 - val_acc: 0.8182\n",
            "Epoch 60/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0519 - acc: 0.9493 - val_loss: 0.1432 - val_acc: 0.8182\n",
            "Epoch 61/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0516 - acc: 0.9493 - val_loss: 0.1374 - val_acc: 0.8364\n",
            "Epoch 62/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0511 - acc: 0.9447 - val_loss: 0.1401 - val_acc: 0.8182\n",
            "Epoch 63/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0520 - acc: 0.9447 - val_loss: 0.1431 - val_acc: 0.8364\n",
            "Epoch 64/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0532 - acc: 0.9447 - val_loss: 0.1447 - val_acc: 0.8182\n",
            "Epoch 65/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0505 - acc: 0.9493 - val_loss: 0.1464 - val_acc: 0.8182\n",
            "Epoch 66/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0502 - acc: 0.9493 - val_loss: 0.1475 - val_acc: 0.8182\n",
            "Epoch 67/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0499 - acc: 0.9493 - val_loss: 0.1465 - val_acc: 0.8182\n",
            "Epoch 68/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0498 - acc: 0.9493 - val_loss: 0.1476 - val_acc: 0.8182\n",
            "Epoch 69/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0496 - acc: 0.9493 - val_loss: 0.1463 - val_acc: 0.8182\n",
            "Epoch 70/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0495 - acc: 0.9493 - val_loss: 0.1468 - val_acc: 0.8182\n",
            "Epoch 71/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0495 - acc: 0.9493 - val_loss: 0.1484 - val_acc: 0.8182\n",
            "Epoch 72/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0493 - acc: 0.9493 - val_loss: 0.1480 - val_acc: 0.8182\n",
            "Epoch 73/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0495 - acc: 0.9493 - val_loss: 0.1498 - val_acc: 0.8182\n",
            "Epoch 74/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0500 - acc: 0.9447 - val_loss: 0.1493 - val_acc: 0.8182\n",
            "Epoch 75/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0490 - acc: 0.9493 - val_loss: 0.1502 - val_acc: 0.8182\n",
            "Epoch 76/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0491 - acc: 0.9493 - val_loss: 0.1477 - val_acc: 0.8182\n",
            "Epoch 77/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0490 - acc: 0.9493 - val_loss: 0.1479 - val_acc: 0.8182\n",
            "Epoch 78/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0489 - acc: 0.9493 - val_loss: 0.1513 - val_acc: 0.8000\n",
            "Epoch 79/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0488 - acc: 0.9493 - val_loss: 0.1507 - val_acc: 0.8182\n",
            "Epoch 80/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0488 - acc: 0.9493 - val_loss: 0.1507 - val_acc: 0.8182\n",
            "Epoch 81/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0488 - acc: 0.9493 - val_loss: 0.1505 - val_acc: 0.8182\n",
            "Epoch 82/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0486 - acc: 0.9493 - val_loss: 0.1502 - val_acc: 0.8182\n",
            "Epoch 83/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0486 - acc: 0.9493 - val_loss: 0.1515 - val_acc: 0.8000\n",
            "Epoch 84/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0494 - acc: 0.9447 - val_loss: 0.1518 - val_acc: 0.8000\n",
            "Epoch 85/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0482 - acc: 0.9493 - val_loss: 0.1537 - val_acc: 0.8182\n",
            "Epoch 86/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0483 - acc: 0.9493 - val_loss: 0.1530 - val_acc: 0.8000\n",
            "Epoch 87/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0477 - acc: 0.9493 - val_loss: 0.1577 - val_acc: 0.8182\n",
            "Epoch 88/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0454 - acc: 0.9539 - val_loss: 0.1642 - val_acc: 0.8000\n",
            "Epoch 89/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0447 - acc: 0.9539 - val_loss: 0.1639 - val_acc: 0.8000\n",
            "Epoch 90/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0439 - acc: 0.9539 - val_loss: 0.1621 - val_acc: 0.8000\n",
            "Epoch 91/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0436 - acc: 0.9539 - val_loss: 0.1623 - val_acc: 0.7818\n",
            "Epoch 92/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0432 - acc: 0.9539 - val_loss: 0.1611 - val_acc: 0.7818\n",
            "Epoch 93/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0447 - acc: 0.9539 - val_loss: 0.1696 - val_acc: 0.7455\n",
            "Epoch 94/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0433 - acc: 0.9539 - val_loss: 0.1676 - val_acc: 0.8000\n",
            "Epoch 95/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0426 - acc: 0.9539 - val_loss: 0.1600 - val_acc: 0.8000\n",
            "Epoch 96/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0426 - acc: 0.9539 - val_loss: 0.1654 - val_acc: 0.7818\n",
            "Epoch 97/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0425 - acc: 0.9539 - val_loss: 0.1598 - val_acc: 0.8000\n",
            "Epoch 98/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0414 - acc: 0.9539 - val_loss: 0.1602 - val_acc: 0.8000\n",
            "Epoch 99/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0421 - acc: 0.9539 - val_loss: 0.1601 - val_acc: 0.8000\n",
            "Epoch 100/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0426 - acc: 0.9539 - val_loss: 0.1648 - val_acc: 0.7818\n",
            "Epoch 101/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0418 - acc: 0.9539 - val_loss: 0.1597 - val_acc: 0.8000\n",
            "Epoch 102/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0415 - acc: 0.9539 - val_loss: 0.1592 - val_acc: 0.8000\n",
            "Epoch 103/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0405 - acc: 0.9585 - val_loss: 0.1649 - val_acc: 0.7818\n",
            "Epoch 104/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0406 - acc: 0.9585 - val_loss: 0.1562 - val_acc: 0.8000\n",
            "Epoch 105/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0406 - acc: 0.9585 - val_loss: 0.1640 - val_acc: 0.7818\n",
            "Epoch 106/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0403 - acc: 0.9585 - val_loss: 0.1613 - val_acc: 0.8000\n",
            "Epoch 107/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0402 - acc: 0.9585 - val_loss: 0.1595 - val_acc: 0.8000\n",
            "Epoch 108/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0399 - acc: 0.9585 - val_loss: 0.1635 - val_acc: 0.8000\n",
            "Epoch 109/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0397 - acc: 0.9585 - val_loss: 0.1641 - val_acc: 0.8000\n",
            "Epoch 110/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0396 - acc: 0.9585 - val_loss: 0.1645 - val_acc: 0.8000\n",
            "Epoch 111/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0397 - acc: 0.9585 - val_loss: 0.1640 - val_acc: 0.8000\n",
            "Epoch 112/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0397 - acc: 0.9585 - val_loss: 0.1655 - val_acc: 0.7818\n",
            "Epoch 113/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0395 - acc: 0.9585 - val_loss: 0.1652 - val_acc: 0.8000\n",
            "Epoch 114/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0395 - acc: 0.9585 - val_loss: 0.1651 - val_acc: 0.8000\n",
            "Epoch 115/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0395 - acc: 0.9585 - val_loss: 0.1658 - val_acc: 0.8000\n",
            "Epoch 116/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0395 - acc: 0.9585 - val_loss: 0.1657 - val_acc: 0.8000\n",
            "Epoch 117/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1661 - val_acc: 0.8000\n",
            "Epoch 118/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1667 - val_acc: 0.8000\n",
            "Epoch 119/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1671 - val_acc: 0.8000\n",
            "Epoch 120/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1655 - val_acc: 0.8000\n",
            "Epoch 121/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1658 - val_acc: 0.8000\n",
            "Epoch 122/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1673 - val_acc: 0.8000\n",
            "Epoch 123/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0393 - acc: 0.9585 - val_loss: 0.1673 - val_acc: 0.8000\n",
            "Epoch 124/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0393 - acc: 0.9585 - val_loss: 0.1658 - val_acc: 0.8000\n",
            "Epoch 125/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0393 - acc: 0.9585 - val_loss: 0.1665 - val_acc: 0.8000\n",
            "Epoch 126/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0393 - acc: 0.9585 - val_loss: 0.1669 - val_acc: 0.8000\n",
            "Epoch 127/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0392 - acc: 0.9585 - val_loss: 0.1660 - val_acc: 0.8000\n",
            "Epoch 128/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0392 - acc: 0.9585 - val_loss: 0.1662 - val_acc: 0.8000\n",
            "Epoch 129/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0392 - acc: 0.9585 - val_loss: 0.1669 - val_acc: 0.8000\n",
            "Epoch 130/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0392 - acc: 0.9585 - val_loss: 0.1669 - val_acc: 0.8000\n",
            "Epoch 131/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0392 - acc: 0.9585 - val_loss: 0.1663 - val_acc: 0.8000\n",
            "Epoch 132/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0393 - acc: 0.9585 - val_loss: 0.1666 - val_acc: 0.8000\n",
            "Epoch 133/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1671 - val_acc: 0.8000\n",
            "Epoch 134/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1665 - val_acc: 0.8000\n",
            "Epoch 135/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1668 - val_acc: 0.8000\n",
            "Epoch 136/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1671 - val_acc: 0.8000\n",
            "Epoch 137/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1676 - val_acc: 0.8000\n",
            "Epoch 138/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0393 - acc: 0.9585 - val_loss: 0.1674 - val_acc: 0.8000\n",
            "Epoch 139/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1673 - val_acc: 0.8000\n",
            "Epoch 140/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1678 - val_acc: 0.8000\n",
            "Epoch 141/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1681 - val_acc: 0.8000\n",
            "Epoch 142/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0391 - acc: 0.9585 - val_loss: 0.1685 - val_acc: 0.8000\n",
            "Epoch 143/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0390 - acc: 0.9585 - val_loss: 0.1695 - val_acc: 0.8000\n",
            "Epoch 144/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0390 - acc: 0.9585 - val_loss: 0.1703 - val_acc: 0.7818\n",
            "Epoch 145/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0390 - acc: 0.9585 - val_loss: 0.1712 - val_acc: 0.7818\n",
            "Epoch 146/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0389 - acc: 0.9585 - val_loss: 0.1730 - val_acc: 0.7818\n",
            "Epoch 147/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0390 - acc: 0.9585 - val_loss: 0.1745 - val_acc: 0.7818\n",
            "Epoch 148/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0389 - acc: 0.9585 - val_loss: 0.1759 - val_acc: 0.7818\n",
            "Epoch 149/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0389 - acc: 0.9585 - val_loss: 0.1767 - val_acc: 0.7818\n",
            "Epoch 150/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0389 - acc: 0.9585 - val_loss: 0.1784 - val_acc: 0.7818\n",
            "Epoch 151/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0388 - acc: 0.9585 - val_loss: 0.1783 - val_acc: 0.7818\n",
            "Epoch 152/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0386 - acc: 0.9585 - val_loss: 0.1766 - val_acc: 0.7818\n",
            "Epoch 153/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0382 - acc: 0.9585 - val_loss: 0.1682 - val_acc: 0.8000\n",
            "Epoch 154/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0392 - acc: 0.9539 - val_loss: 0.1758 - val_acc: 0.7818\n",
            "Epoch 155/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0389 - acc: 0.9539 - val_loss: 0.1741 - val_acc: 0.8000\n",
            "Epoch 156/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0376 - acc: 0.9585 - val_loss: 0.1686 - val_acc: 0.8000\n",
            "Epoch 157/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0362 - acc: 0.9585 - val_loss: 0.1836 - val_acc: 0.7818\n",
            "Epoch 158/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0367 - acc: 0.9631 - val_loss: 0.1704 - val_acc: 0.8000\n",
            "Epoch 159/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0378 - acc: 0.9585 - val_loss: 0.1776 - val_acc: 0.8000\n",
            "Epoch 160/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0356 - acc: 0.9631 - val_loss: 0.1755 - val_acc: 0.8000\n",
            "Epoch 161/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0351 - acc: 0.9631 - val_loss: 0.1777 - val_acc: 0.8000\n",
            "Epoch 162/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0351 - acc: 0.9631 - val_loss: 0.1775 - val_acc: 0.8000\n",
            "Epoch 163/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0350 - acc: 0.9631 - val_loss: 0.1777 - val_acc: 0.8000\n",
            "Epoch 164/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0351 - acc: 0.9631 - val_loss: 0.1780 - val_acc: 0.7818\n",
            "Epoch 165/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0350 - acc: 0.9631 - val_loss: 0.1776 - val_acc: 0.8000\n",
            "Epoch 166/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0350 - acc: 0.9631 - val_loss: 0.1792 - val_acc: 0.7818\n",
            "Epoch 167/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0349 - acc: 0.9631 - val_loss: 0.1794 - val_acc: 0.7818\n",
            "Epoch 168/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0349 - acc: 0.9631 - val_loss: 0.1787 - val_acc: 0.7818\n",
            "Epoch 169/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0349 - acc: 0.9631 - val_loss: 0.1796 - val_acc: 0.7818\n",
            "Epoch 170/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0349 - acc: 0.9631 - val_loss: 0.1802 - val_acc: 0.7818\n",
            "Epoch 171/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0348 - acc: 0.9631 - val_loss: 0.1801 - val_acc: 0.7818\n",
            "Epoch 172/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0348 - acc: 0.9631 - val_loss: 0.1801 - val_acc: 0.7818\n",
            "Epoch 173/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0348 - acc: 0.9631 - val_loss: 0.1799 - val_acc: 0.7818\n",
            "Epoch 174/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0347 - acc: 0.9631 - val_loss: 0.1788 - val_acc: 0.7818\n",
            "Epoch 175/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0347 - acc: 0.9631 - val_loss: 0.1802 - val_acc: 0.7818\n",
            "Epoch 176/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0346 - acc: 0.9631 - val_loss: 0.1812 - val_acc: 0.7818\n",
            "Epoch 177/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0346 - acc: 0.9631 - val_loss: 0.1813 - val_acc: 0.7818\n",
            "Epoch 178/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0346 - acc: 0.9631 - val_loss: 0.1818 - val_acc: 0.7818\n",
            "Epoch 179/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0345 - acc: 0.9631 - val_loss: 0.1817 - val_acc: 0.7818\n",
            "Epoch 180/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0345 - acc: 0.9631 - val_loss: 0.1824 - val_acc: 0.7818\n",
            "Epoch 181/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0344 - acc: 0.9631 - val_loss: 0.1833 - val_acc: 0.7818\n",
            "Epoch 182/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0344 - acc: 0.9631 - val_loss: 0.1839 - val_acc: 0.7818\n",
            "Epoch 183/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0344 - acc: 0.9631 - val_loss: 0.1844 - val_acc: 0.7818\n",
            "Epoch 184/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0343 - acc: 0.9631 - val_loss: 0.1834 - val_acc: 0.7818\n",
            "Epoch 185/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0342 - acc: 0.9631 - val_loss: 0.1853 - val_acc: 0.7818\n",
            "Epoch 186/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0341 - acc: 0.9631 - val_loss: 0.1847 - val_acc: 0.7818\n",
            "Epoch 187/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0341 - acc: 0.9631 - val_loss: 0.1861 - val_acc: 0.7818\n",
            "Epoch 188/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0340 - acc: 0.9631 - val_loss: 0.1872 - val_acc: 0.7818\n",
            "Epoch 189/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0339 - acc: 0.9631 - val_loss: 0.1875 - val_acc: 0.7818\n",
            "Epoch 190/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0339 - acc: 0.9631 - val_loss: 0.1865 - val_acc: 0.7818\n",
            "Epoch 191/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0339 - acc: 0.9631 - val_loss: 0.1882 - val_acc: 0.7818\n",
            "Epoch 192/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0337 - acc: 0.9631 - val_loss: 0.1884 - val_acc: 0.7818\n",
            "Epoch 193/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0337 - acc: 0.9631 - val_loss: 0.1897 - val_acc: 0.7818\n",
            "Epoch 194/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0336 - acc: 0.9631 - val_loss: 0.1900 - val_acc: 0.7818\n",
            "Epoch 195/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0335 - acc: 0.9631 - val_loss: 0.1918 - val_acc: 0.7818\n",
            "Epoch 196/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0334 - acc: 0.9631 - val_loss: 0.1934 - val_acc: 0.7818\n",
            "Epoch 197/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0334 - acc: 0.9631 - val_loss: 0.1937 - val_acc: 0.7818\n",
            "Epoch 198/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0333 - acc: 0.9631 - val_loss: 0.1948 - val_acc: 0.7818\n",
            "Epoch 199/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0333 - acc: 0.9631 - val_loss: 0.1940 - val_acc: 0.7818\n",
            "Epoch 200/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0331 - acc: 0.9631 - val_loss: 0.1970 - val_acc: 0.7818\n",
            "Epoch 201/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0330 - acc: 0.9631 - val_loss: 0.1977 - val_acc: 0.7818\n",
            "Epoch 202/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0330 - acc: 0.9631 - val_loss: 0.1978 - val_acc: 0.7818\n",
            "Epoch 203/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0329 - acc: 0.9631 - val_loss: 0.1989 - val_acc: 0.7818\n",
            "Epoch 204/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0328 - acc: 0.9631 - val_loss: 0.1978 - val_acc: 0.7818\n",
            "Epoch 205/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0328 - acc: 0.9631 - val_loss: 0.1993 - val_acc: 0.7818\n",
            "Epoch 206/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0326 - acc: 0.9631 - val_loss: 0.1996 - val_acc: 0.7818\n",
            "Epoch 207/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0324 - acc: 0.9631 - val_loss: 0.1953 - val_acc: 0.7818\n",
            "Epoch 208/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0322 - acc: 0.9631 - val_loss: 0.1974 - val_acc: 0.7818\n",
            "Epoch 209/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0319 - acc: 0.9631 - val_loss: 0.2024 - val_acc: 0.7818\n",
            "Epoch 210/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0317 - acc: 0.9631 - val_loss: 0.2026 - val_acc: 0.7818\n",
            "Epoch 211/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0314 - acc: 0.9631 - val_loss: 0.2027 - val_acc: 0.7818\n",
            "Epoch 212/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0313 - acc: 0.9631 - val_loss: 0.2002 - val_acc: 0.7818\n",
            "Epoch 213/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0310 - acc: 0.9631 - val_loss: 0.1971 - val_acc: 0.7818\n",
            "Epoch 214/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0306 - acc: 0.9631 - val_loss: 0.1994 - val_acc: 0.7818\n",
            "Epoch 215/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0304 - acc: 0.9631 - val_loss: 0.1957 - val_acc: 0.7818\n",
            "Epoch 216/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0301 - acc: 0.9631 - val_loss: 0.1978 - val_acc: 0.7818\n",
            "Epoch 217/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0294 - acc: 0.9631 - val_loss: 0.1945 - val_acc: 0.7818\n",
            "Epoch 218/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0302 - acc: 0.9631 - val_loss: 0.1958 - val_acc: 0.7818\n",
            "Epoch 219/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0288 - acc: 0.9631 - val_loss: 0.2026 - val_acc: 0.7818\n",
            "Epoch 220/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0288 - acc: 0.9631 - val_loss: 0.2025 - val_acc: 0.7818\n",
            "Epoch 221/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0283 - acc: 0.9631 - val_loss: 0.2047 - val_acc: 0.7818\n",
            "Epoch 222/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0274 - acc: 0.9631 - val_loss: 0.2017 - val_acc: 0.7818\n",
            "Epoch 223/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0272 - acc: 0.9631 - val_loss: 0.2044 - val_acc: 0.7818\n",
            "Epoch 224/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0272 - acc: 0.9631 - val_loss: 0.1916 - val_acc: 0.7818\n",
            "Epoch 225/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0284 - acc: 0.9631 - val_loss: 0.2190 - val_acc: 0.7636\n",
            "Epoch 226/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0276 - acc: 0.9631 - val_loss: 0.1784 - val_acc: 0.8000\n",
            "Epoch 227/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0307 - acc: 0.9539 - val_loss: 0.2012 - val_acc: 0.7818\n",
            "Epoch 228/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0264 - acc: 0.9631 - val_loss: 0.2037 - val_acc: 0.7818\n",
            "Epoch 229/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0253 - acc: 0.9677 - val_loss: 0.2022 - val_acc: 0.7818\n",
            "Epoch 230/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0256 - acc: 0.9631 - val_loss: 0.2045 - val_acc: 0.7818\n",
            "Epoch 231/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0246 - acc: 0.9724 - val_loss: 0.2044 - val_acc: 0.7818\n",
            "Epoch 232/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0238 - acc: 0.9816 - val_loss: 0.2078 - val_acc: 0.7818\n",
            "Epoch 233/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0239 - acc: 0.9677 - val_loss: 0.2064 - val_acc: 0.7818\n",
            "Epoch 234/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0234 - acc: 0.9770 - val_loss: 0.2108 - val_acc: 0.7636\n",
            "Epoch 235/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0230 - acc: 0.9816 - val_loss: 0.2116 - val_acc: 0.7636\n",
            "Epoch 236/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0226 - acc: 0.9816 - val_loss: 0.2113 - val_acc: 0.7636\n",
            "Epoch 237/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0223 - acc: 0.9816 - val_loss: 0.2100 - val_acc: 0.7636\n",
            "Epoch 238/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.2108 - val_acc: 0.7636\n",
            "Epoch 239/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.2176 - val_acc: 0.7636\n",
            "Epoch 240/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0223 - acc: 0.9816 - val_loss: 0.2099 - val_acc: 0.7636\n",
            "Epoch 241/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.2141 - val_acc: 0.7636\n",
            "Epoch 242/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0232 - acc: 0.9770 - val_loss: 0.2109 - val_acc: 0.7636\n",
            "Epoch 243/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0209 - acc: 0.9816 - val_loss: 0.2171 - val_acc: 0.7636\n",
            "Epoch 244/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0207 - acc: 0.9816 - val_loss: 0.2169 - val_acc: 0.7636\n",
            "Epoch 245/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0204 - acc: 0.9816 - val_loss: 0.2163 - val_acc: 0.7636\n",
            "Epoch 246/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0204 - acc: 0.9816 - val_loss: 0.2159 - val_acc: 0.7636\n",
            "Epoch 247/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0201 - acc: 0.9816 - val_loss: 0.2174 - val_acc: 0.7636\n",
            "Epoch 248/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0200 - acc: 0.9816 - val_loss: 0.2172 - val_acc: 0.7636\n",
            "Epoch 249/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0199 - acc: 0.9816 - val_loss: 0.2166 - val_acc: 0.7636\n",
            "Epoch 250/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0200 - acc: 0.9816 - val_loss: 0.2197 - val_acc: 0.7636\n",
            "Epoch 251/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0198 - acc: 0.9816 - val_loss: 0.2160 - val_acc: 0.7636\n",
            "Epoch 252/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0195 - acc: 0.9816 - val_loss: 0.2189 - val_acc: 0.7636\n",
            "Epoch 253/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0195 - acc: 0.9816 - val_loss: 0.2179 - val_acc: 0.7636\n",
            "Epoch 254/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0194 - acc: 0.9816 - val_loss: 0.2200 - val_acc: 0.7636\n",
            "Epoch 255/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0192 - acc: 0.9816 - val_loss: 0.2179 - val_acc: 0.7636\n",
            "Epoch 256/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0191 - acc: 0.9816 - val_loss: 0.2201 - val_acc: 0.7636\n",
            "Epoch 257/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0191 - acc: 0.9816 - val_loss: 0.2196 - val_acc: 0.7636\n",
            "Epoch 258/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0191 - acc: 0.9816 - val_loss: 0.2192 - val_acc: 0.7636\n",
            "Epoch 259/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0190 - acc: 0.9816 - val_loss: 0.2195 - val_acc: 0.7636\n",
            "Epoch 260/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0189 - acc: 0.9816 - val_loss: 0.2189 - val_acc: 0.7636\n",
            "Epoch 261/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0188 - acc: 0.9816 - val_loss: 0.2197 - val_acc: 0.7636\n",
            "Epoch 262/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0188 - acc: 0.9816 - val_loss: 0.2191 - val_acc: 0.7636\n",
            "Epoch 263/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0187 - acc: 0.9816 - val_loss: 0.2211 - val_acc: 0.7636\n",
            "Epoch 264/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0187 - acc: 0.9816 - val_loss: 0.2204 - val_acc: 0.7636\n",
            "Epoch 265/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0187 - acc: 0.9816 - val_loss: 0.2196 - val_acc: 0.7636\n",
            "Epoch 266/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0187 - acc: 0.9816 - val_loss: 0.2193 - val_acc: 0.7636\n",
            "Epoch 267/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0186 - acc: 0.9816 - val_loss: 0.2208 - val_acc: 0.7636\n",
            "Epoch 268/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0186 - acc: 0.9816 - val_loss: 0.2205 - val_acc: 0.7636\n",
            "Epoch 269/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0186 - acc: 0.9816 - val_loss: 0.2208 - val_acc: 0.7636\n",
            "Epoch 270/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0185 - acc: 0.9816 - val_loss: 0.2206 - val_acc: 0.7636\n",
            "Epoch 271/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0185 - acc: 0.9816 - val_loss: 0.2209 - val_acc: 0.7636\n",
            "Epoch 272/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0185 - acc: 0.9816 - val_loss: 0.2208 - val_acc: 0.7636\n",
            "Epoch 273/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0185 - acc: 0.9816 - val_loss: 0.2211 - val_acc: 0.7636\n",
            "Epoch 274/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0185 - acc: 0.9816 - val_loss: 0.2208 - val_acc: 0.7636\n",
            "Epoch 275/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2210 - val_acc: 0.7636\n",
            "Epoch 276/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2212 - val_acc: 0.7636\n",
            "Epoch 277/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2214 - val_acc: 0.7636\n",
            "Epoch 278/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2214 - val_acc: 0.7636\n",
            "Epoch 279/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2212 - val_acc: 0.7636\n",
            "Epoch 280/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2214 - val_acc: 0.7636\n",
            "Epoch 281/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.2215 - val_acc: 0.7636\n",
            "Epoch 282/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2212 - val_acc: 0.7636\n",
            "Epoch 283/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2207 - val_acc: 0.7636\n",
            "Epoch 284/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 285/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2219 - val_acc: 0.7636\n",
            "Epoch 286/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2213 - val_acc: 0.7636\n",
            "Epoch 287/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2218 - val_acc: 0.7636\n",
            "Epoch 288/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.2214 - val_acc: 0.7636\n",
            "Epoch 289/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2217 - val_acc: 0.7636\n",
            "Epoch 290/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2221 - val_acc: 0.7636\n",
            "Epoch 291/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2219 - val_acc: 0.7636\n",
            "Epoch 292/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2217 - val_acc: 0.7636\n",
            "Epoch 293/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2216 - val_acc: 0.7636\n",
            "Epoch 294/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2218 - val_acc: 0.7636\n",
            "Epoch 295/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2218 - val_acc: 0.7636\n",
            "Epoch 296/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.7636\n",
            "Epoch 297/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2218 - val_acc: 0.7636\n",
            "Epoch 298/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 299/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.7636\n",
            "Epoch 300/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 301/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n",
            "Epoch 302/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2219 - val_acc: 0.7636\n",
            "Epoch 303/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 304/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.7636\n",
            "Epoch 305/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2218 - val_acc: 0.7636\n",
            "Epoch 306/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2221 - val_acc: 0.7636\n",
            "Epoch 307/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2219 - val_acc: 0.7636\n",
            "Epoch 308/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 309/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2221 - val_acc: 0.7636\n",
            "Epoch 310/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.7636\n",
            "Epoch 311/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.7636\n",
            "Epoch 312/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2218 - val_acc: 0.7636\n",
            "Epoch 313/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 314/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n",
            "Epoch 315/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 316/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 317/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n",
            "Epoch 318/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 319/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 320/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 321/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 322/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 323/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 324/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 325/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 326/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 327/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n",
            "Epoch 328/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 329/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 330/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 331/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 332/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 333/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n",
            "Epoch 334/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 335/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 336/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 337/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 338/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 339/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 340/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 341/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 342/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 343/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 344/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 345/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 346/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 347/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 348/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 349/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 350/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 351/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 352/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 353/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 354/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 355/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2230 - val_acc: 0.7636\n",
            "Epoch 356/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 357/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 358/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2230 - val_acc: 0.7636\n",
            "Epoch 359/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 360/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2229 - val_acc: 0.7636\n",
            "Epoch 361/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2230 - val_acc: 0.7636\n",
            "Epoch 362/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 363/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 364/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 365/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 366/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 367/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2229 - val_acc: 0.7636\n",
            "Epoch 368/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 369/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 370/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2231 - val_acc: 0.7636\n",
            "Epoch 371/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2231 - val_acc: 0.7636\n",
            "Epoch 372/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 373/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 374/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 375/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 376/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 377/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2231 - val_acc: 0.7636\n",
            "Epoch 378/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 379/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 380/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 381/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 382/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.7636\n",
            "Epoch 383/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 384/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 385/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 386/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 387/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.2223 - val_acc: 0.7636\n",
            "Epoch 388/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2229 - val_acc: 0.7636\n",
            "Epoch 389/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 390/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 391/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 392/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 393/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 394/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 395/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 396/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 397/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2230 - val_acc: 0.7636\n",
            "Epoch 398/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 399/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2220 - val_acc: 0.7636\n",
            "Epoch 400/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2224 - val_acc: 0.7636\n",
            "Epoch 401/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2221 - val_acc: 0.7636\n",
            "Epoch 402/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2221 - val_acc: 0.7636\n",
            "Epoch 403/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2225 - val_acc: 0.7636\n",
            "Epoch 404/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2226 - val_acc: 0.7636\n",
            "Epoch 405/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2229 - val_acc: 0.7636\n",
            "Epoch 406/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n",
            "Epoch 407/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2227 - val_acc: 0.7636\n",
            "Epoch 408/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.2222 - val_acc: 0.7636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 12. What does the plot generated by this code represent?\n",
        "# to visualise how well the model learning run\n",
        "\n",
        "plt.plot(output.history['acc'])\n",
        "plt.plot(output.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "#plt.savefig('Accuracy.png',dpi=100) #to save the image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ckXHLazuf3UD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "03fcd1b5-b82a-4938-bb10-5ec65fdbda1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 13. Plot code for the model loss. You can refer to the plot code for model accuracy above.\n"
      ],
      "metadata": {
        "id": "eHwucWXhf3bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 14. What is the purpose of evaluating the model on the test dataset?\n",
        "\n",
        "#model.load_weights(model_loc+\"heart_disease_best_model.hdf5\")\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]))\n",
        "print(\"loss:\", round(scores[0],2))"
      ],
      "metadata": {
        "id": "d3UkVfYAf3gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70558254-4de5-41c2-a0fd-6420b2b94a25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2181 - acc: 0.7742\n",
            "\n",
            "acc: 77.42%\n",
            "loss: 0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display detailed prediction\n",
        "pred = model.predict(x_test)\n",
        "y = np.round(pred).astype(\"int16\")\n",
        "idx = 0\n",
        "ps = 0\n",
        "fl = 0\n",
        "for x in pred:\n",
        "    if y_test[idx]==y[idx]:\n",
        "        print(\"\\033[30mNo:\",idx+1,\"Actual:\",y_test[idx],\" Predicted:\",y[idx],\"Result: \\033[92mPass\")\n",
        "        ps = ps+1\n",
        "    else:\n",
        "        print(\"\\033[30mNo:\",idx+1,\"Actual:\",y_test[idx],\" Predicted:\",y[idx],\" Result: \\033[91mFail\")\n",
        "        fl = fl+1\n",
        "    idx = idx + 1\n",
        "print(\"\\033[30mRight Prediction :\",ps, \"Wrong Prediction :\",fl)"
      ],
      "metadata": {
        "id": "aqYVf5z0gITK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0d5324-0365-44fd-a342-d4e9bacaed71"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 104ms/step\n",
            "\u001b[30mNo: 1 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 2 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 3 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 4 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 5 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 6 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 7 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 8 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 9 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 10 Actual: [1]  Predicted: [0]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 11 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 12 Actual: [1]  Predicted: [0]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 13 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 14 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 15 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 16 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 17 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 18 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 19 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 20 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 21 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 22 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 23 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 24 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 25 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 26 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 27 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 28 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 29 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 30 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 31 Actual: [1]  Predicted: [0]  Result: \u001b[91mFail\n",
            "\u001b[30mRight Prediction : 24 Wrong Prediction : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 15. What is Confusion Matrix and why you need it? Explain TP, FP, FN, TN.\n",
        "### 16. Explain the classification report produce.\n",
        "\n",
        "y_pred = y\n",
        "y_true = y_test\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "#cm = confusion_matrix(y_true, y_pred, labels=labels.astype('int'))\n",
        "f, ax=plt.subplots(figsize=(5,5))\n",
        "sns.heatmap(cm,annot=True,linewidths=1.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
        "plt.xlabel(\"y_pred\")\n",
        "plt.ylabel(\"y_true\")\n",
        "plt.show()\n",
        "print()\n",
        "print(classification_report(y_true, y_pred, labels=[0,1]))"
      ],
      "metadata": {
        "id": "W6LItiSngKA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "955d2c0e-fa72-4099-81ea-eb51472636f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.71      0.74        14\n",
            "           1       0.78      0.82      0.80        17\n",
            "\n",
            "    accuracy                           0.77        31\n",
            "   macro avg       0.77      0.77      0.77        31\n",
            "weighted avg       0.77      0.77      0.77        31\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}